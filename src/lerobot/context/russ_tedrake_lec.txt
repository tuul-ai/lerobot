

--- YouTube Transcript ---

Yeah. So, this is going to sound a little different than most of the large model talks you maybe here. If I do it right, it might even sound almost like a biology talk or something like this. I think we're building we all kind of know this scaling hypothesis is working or many of us believe that the scaling hypothesis is working but I don't think we really understand the mechanisms and we're um we're building systems that are very complex. They're doing something cool in control but we don't really understand what they're doing. So I want to take you on a little journey that we've been on to kind of dig try to dig a little deeper understand what we can actually understand uh you know and say and and and learn from these big systems that we're starting to build. So the diffusion policy uh maybe you know thank you and u uh that's what that does it's amazing it works really really well. We think of that when I'll refer to diffusion policy today, we'll refer to that as a single task uh uh imitation learning pipeline. So if you want to fold a t-shirt, you do you telly operate the robot to do the to fold the t-shirt 200 times is our standard more if it's an important demo. Um okay and the thing you get out is a visual motor policy. So we have vision encoders in robot joints or endeector encoding in and action decoding out. And now we use this term large behavior models LBMs for the multitask version. Right? I'll compare I'll compare what I mean to that with VAS and everything in a minute. So this is just uh imagine we've been doing diffusion policy at DR for a couple years now. We've done a lot of demonstrations. Uh we've tried to to be very broad in our curriculum. uh we're going to put all of those together, all the data we've collected. Uh and we're going to combine OXD and other internet data and put it all into this u one big model, right? So we're going to take all of our robot training data, all of our simulation data, all of our online data, put that into one big multitask model. Of course, since it's since we can nowadays, we're going to make this language conditioned and come up with a language conditioned visual motor policy. So this is standard fair. In my words, uh the opportunity here is is awesome, right? First of all, we're seeing robots do things with their hands that just weren't possible before, uh including cloth, liquids, and other things like that. There's a potential to program them via imprecise natural language andor a few demonstrations, and we'll talk exactly about sort of that in a minute. And maybe the most subtle thing which I don't think we really understand yet is this idea that if you can get something like the common sense we got in large language models for physical intelligence then that should lead to something like physical like robustness in our policies right something like you know robust control was trying to give us before this is a very different route to a similar end so let's try to understand that and for me personally this is a big deal right I mean we've been I've been thinking about control and dynamics for a long time. This is pretty different than the way we used to talk about it. And u but I don't think it's I think the objects are the same. Um when we understand it really deeply, I think we're going to say they're more similar than different. Okay. So let's um let's do a starter example here. This is uh two sideby-side videos. Uh on the on the one side I've got the single task version. So it's trained as a diffusion policy. Although the robot is doing many steps here, this is actually trained as a single task. So every demonstration was given for the entire sequence of making an entire breakfast scenario. Okay. Um it's 229 demonstrations because we do some quality assurance and the 229 are what's what survived. Okay. Um, and on the right we have uh the same task, but instead of just training on the 229 demonstrations, we take a large behavior model that was pre-trained on all of our existing uh demonstrations and then just fine-tuned with exactly the same 229 demonstrations. Okay, so that's the one of the contrasts I want to study with you today. And they both do remarkably well. This is actually not that cherrypicked. I we'll show you the the statistics. Um there's something just a bit better about the way that the large the pre-training sort of pre-trained models rolls out. Okay, you can see it in the success rate. You can see it into some more subtle rubrics about how often it pauses, how smooth are the trajectories. Okay, and we I want to unpack that and there's something that's making these things better. We felt we're making these things better. Okay, but we're biased. We've been, you know, trying pretty hard to build these models. Here's another fun one that we did uh just recently. So, it's trained again as a single long task. This is one that really um performs much better once we have the pre-trained model. And it's a level of dexterity that um I don't know, it still surprises me. It's just awesome to watch it work. So, this isn't even the best roll out, but it because it misses the middle of the apple. I'll show you our good and our bad today. But, I like this one. Does some awesome little recovery in the middle. So you'll see some of the the robustness that pops out. And who doesn't love robots with knives, right? So there's there's a lot of protection around this just for the we are safe. Okay. So I mean if you've done kinematics, okay, having a long appendage like this and trying to do fine manipulation at the end of a long appendage, that's a pretty complicated fine motor task. Okay. um go through and do the do the job. It's going to chop up the apple. But I want to draw your attent piece. Okay. Little things that happen that just are just amazing. Okay. So, this one right here, it kind of pops off. Those little recovery maneuvers are just like, oh, I love that. I love that. And you can find some things like that in the single task training for this behavior, but they pop out. We think they pop out more. We're quantifying this. And I'll show you the kind of data we're and the way we're quantifying this. And just because it's a fun I'll let it just play out here. Got to clean the knife because we cut a lot of apples this that day. and then even sheath the knife, which is when it fails, it normally fails at that step. I got to tell you, that's the hard one, right? Okay. So, this is just a a level of dexterity that I didn't expect from our robots a few years ago. And and I'll show you some time lapse. This is it's not doesn't always succeed, but it's pretty good. It's pretty good. Okay. So, let me just ask the most basic question here. Does all this pre-training that we're all talking about, does it really help? If I just care about the quality of my dextrous manipulation, is pre-training a path to making that really work well? It's such a basic statement. I think we all believe it to be true, you know, but I I really want to come at you, you know, saying that we're biased, right? I'm biased. I want it to be true so much, but I don't want I've been shy to talk about it until I was very confident that we had evidence to support that it's true, you know, and I don't want to I mean, I think we're in a time of great excitement, of great hype, and I want to, you know, I want to be leave keep myself to the highest possible standards so that hopefully you believe me when I say the things I say. Do we actually have evidence that pre-training makes dextrous manipulation better? Right? If yes, how precisely is it better? Let's get another level of detail. And on the practical side, maybe you already believe that and you don't um need to be convinced, but I think a lot of the things we'll talk about today are just about building the right tools to be able to even make the decisions that you need to make quickly. If you're iterating over policy architectures or data sets or whatever like this, do we have the tools to make those design decisions quickly? So, I want to as a scientist, I want to sort of um tell you the way the hypotheses as I wrote them down like a year and a half ago, right? The the way you're supposed to do science is you say your hypothesis, you lock them in a box, and you're not allowed to change them after you've seen your data. Okay? So, I I regret a few things about this, but it's close. And so, let me just talk you through this. So the first thing I wanted to see in the multitask scaling laws was that if I train a diffusion policy for instance, and it's my first ever diffusion policy, the first skill I've ever trained, the first task I've ever trained, then maybe it takes about 150 demonstrations to get 80% success rate. Of course, it depends a lot on what task and everything like this. But if I'm training, if I've already trained a thousand tasks, then you'd like to say that I could add a new task with many fewer demonstrations, right? So maybe if I've trained enough then maybe I just give a single demonstration. The model understands what I want and that's enough to fully do the task. And the hope was that actually it's not a step change and you just have to keep giving demonstrations until you reach like robot enlightenment, right? That somehow there's a smooth glide path that somehow we get a nice scaling law that sort of says that the more skills we have, the better we get. So that was the first thing we look for. The second one is a statement about robustness. Of course, you'd like to think that that even if you got only 80% success rate on your first skill, that when you've trained a lot of skills, you'd like to see that the robustness goes up. We're getting used to seeing, you know, low success rates in our learning. But maybe the path to real robustness is actually by large scale pre-training on diverse tasks. And um and that's a, you know, an examination where we go and look for recovery that sort of transfers from other tasks. Okay. Okay, the basic idea is well maybe even the stronger version of of the statement would be imagine you just cared about one task. You're like you're a small business. You're starting you just have one thing you really want the robot to do well and you've got a budget. You say you can afford 10,000 demonstrations. Are you better off doing 10,000 demonstrations of that single task you care about? Or are you better off doing a diversity of tasks and hoping for transfer under that same budget? Right? That's a that's a deep question that I don't think we have a good answer for. And the reason I believe it could be true, I mean, I remember in the days of autonomous when I was working on at on autonomous driving, we talked all about the long tales, right? So you you got lots and lots of highway miles, lots of lots of highway miles. And that's data doesn't um do you that that's not the most valuable data. You wait a long time to get these rare events, right? And in manipulation, I think we have a different opportunity. We don't have to do just the one task. We can actually explore the space by doing different tasks which kind of excite motions and recoveries in different parts of the space which can then be synthesized in a way that could potentially provide sort of the longtail coverage on the task you care about. So that's kind of the idea. All right. So let me again try to be a little bit scientific here. Let me just tell you the method that we're going to use to get to dig into some of these questions. So, we're playing with a couple different types of robots now, but the main focus has still been on the bimanual manipulation. The these two bmanual pandas you see with the the grippers and the wrist cameras, scene cameras. We do all kinds of cool tasks. I'll show you the bike rotor at the end if we have time. Okay. Um, let's talk about the architectures. There's a handful of different ways that you could build a large behavior model. I I use large behavior model. we have used at TRRI for a couple years now the the term large behavior model as a kind of an umbrella of this um you know visually language condition visual motor policy and you could do that with just scaling up diffusion policy you could stick a language encoding on the front of a diffusion policy maybe scale up the parameters a bit uh and and that is a large behavior model you could do a vision language action model this is you know the the name is being used a lot I would try to just put put a sort of I call any model that's using VLM pre-training or co-raining and I would call that a VLA and I I try to use different words for things that were not train pre-trained with a VLM sort of recipe for instance another base model that you could start with is a video prediction model right and that's trained sort of fundamentally differently although of course it's inevitable that VLMs and VLMs will come together in the not too distant future so this slide will expire before the next time I put it in my lecture notes here. Okay. NT and TR actually has a strong team working on video models. So, this is one we take very seriously. We're trying to build uh video model backends also. But just because we've gotten as as you know very far in these specific questions, we decided I decided today to talk about this uh multitask diffusion policy version. It's actually the one I showed you in those amazing uh sort of examples at the beginning. So, let's tell you how I scaled it up. We how we scaled it up is from the diffusion policy. So ResNet is now clip that's not surprising. We're using a clip language encoding encoding. It's mostly frozen. We you know just apart from the um the layers the connect connectivity tissue, right? Uh we're using DIT now instead of ENET. Okay. But actually we're not that much bigger in terms of number of parameters when you add it all up. Nowadays we we tend to share the image encoders when we used to have separate image encoders. There's a pile of tricks which I'm happy to talk about that we've sort of learned over the years. Um, but we're going to talk about training hundreds and hundreds of skills with a model that's only a few times bigger than what we had in diffusion policy. Uh, and the reason for that, I think 150 million was a lot of parameters for diffusion policy. And that was probably more about the optimization landscape than about the capacity of the model. Okay. So, I'll focus on this one because it is there's nothing that outperforms it in terms of those the dextrous capabilities in in our lab. Okay? It's it's one of the best. Um, and it's just fundamentally simpler to study if you're really trying to dig in and say what is the source of these dynamic of, you know, the way my feedback control changed, the way my stability changed, then language complicates that story. And so we're actually there's a there's a second talk I would love to give about the way language enters more deeply into this, but I'm going to defer that. We're not as far I'm not as far along in my thinking with that. And the thing I give up by doing this really is that um these models aren't don't language steer as well as the VA's, right? So sometimes I'll ask it to do something and it'll do a different task that was in the pre-training mix. So um for this type for this set of experiments we locked the data a little while ago when we were at um our high quality long duration 500 skills mark roughly it was a little bit more than 500 skills and it's mostly this real data uh oxe by volume is larger but we batch balance that down to be pretty small uh uh effect on the total training uh and we have this little wedge of simulation data okay which seems weird right if you're if you love simulation then why shouldn't you have a lot of simulation data and that's coming but but not for this experiment you know why is the sim data so small the experiment we're doing here we're actually just using sim basically to help the evaluation okay so we do a lot of real world uh teley operation we have great robot teachers uh and uh we took those same uh imitation learning teley operation pipelines and we applied them in sims we have a corresponding data set in sim We have our set of a small set of of evaluation tasks that we constantly run and really those are just there to carry our real world data into sim so that we can trust our real world sim surrogate for for real world our our sim as a surrogate for real world performance. Okay. So let's dig into like u how we actually do some of the testing. So first of all um AB testing is really really important. So you have to say if I if I tested a policy yesterday that's great I got some results yesterday but if I have a new policy I want to test today and I want to compare it to yet to that other policy I have to rerun that data yesterday is gone basically it's stale something changed about the world I'm always constantly running both policies that I'm trying to compare I can't use reuse the old data always when the evaluation's running you have to run blind randomized trials so the person doing the eval doesn't know if it's a diffusion single task diffusion policy or a multitask um or whatever. Uh we have we we set up guies to sort of help the evaluation to contain and make more consistent the initial condition distributions and the like. So this is just one example. We say we want the scene to look like this. There's an overlay of the desired scene with the actual scene and the evaluators will just snap things into place. This is a little bit of a tax on our our evaluation rate, but it allows us to have a much more controlled set of initial conditions. And then we ask, we don't just ask success rate. We ask about task completion. We ask about, you know, was it um was it jittery? It paused, did it do the wrong task? There's a bunch of things we try to collect that are sort of beyond the success rate. Okay. And then we do a lot of simulationbased testing. This is actually I really don't know how we would do it without this uh at this point. And it's not perfect. I'll tell you some of the nuances, but it's a um it's essential to our workflow. So, basically all the models that are that we're training uh you know, every interval checkpoint, we kick off an automatic eval in the cloud. We get our numbers back. It shows up on the leaderboard. You can scan the videos. You can see what happened. And that extra work of scanning the leader, you know, scanning into the videos and figuring what out what happened. That's a big part of the work. So people have we have different philosophies for different for simulation for different parts of the workflow. So for data generation I think we should be using generative assets. We should be using realto sim. We should be doing all the stuff to get as much diversity and as much content into our simulation as possible. But for testing I use I feel a little differently. We're actually um we're only going to run all of those evaluations on on a handful of scenes. So since we're running so many checkpoints through this I'm actually going to take time. I'm going to let artists touch up make the assets. I'll let the physicist touch up the contact parameters. I want this thing to be bulletproof. No weird policy is going to make some strange physics happen in this simulator. Right? So that's the philosophy is this is the place where you want high quality. You want a low sim to real gap. uh we we tend to come up with handfuls of scenarios to do that and 10 tasks uh greater than 10 tasks per scene so that um the task is not visually obvious and you have to require you have to pay attention to the language in order to accomplish the task and then just like in real we try to just capture about 200 demonstrations per task with a little quality assurance it's 199 or two whatever about 200 demonstrations per task and that again is meant to to bring our real world data into sim. Okay. Um, why do I like simulation for testing? I mean, the fact that it runs automatically on the cloud is is of course a big thing, but the repeatability is so valuable. Anybody who's done real world eval knows how painful it is. Um, and to be able to run a policy last week and then know that that score those scores are still valid this week is just so valuable. Okay. Um also we can just run many many more rollouts. So we get much better statistics of slightly the wrong distribution but we get many much better statistics. Okay. We've done work with Mac and others on on those statistics. Okay. Uh yeah and we run automatically and things just appear magically on the on the leaderboard and it's just it's just a really effective way to have the folks working on the policy to get their results back. strong correspondence with real but not perfect. Right? We work as hard I think as anybody on our simulator. Um but I think if I were to tell you it's a perfect match. I I it's that would be glossing over a lot of nuance. There's just a lot of a lot of small things that could be can can take the policy in simulation on a different path than it does in real. Maybe you don't put enough noise in your joint encoders or something like that. And so it latches on to this like nice clear signal in a way that it wouldn't have done in real. you know, you just you find these, you you knock them down, but there's still a lot of them. Okay, so um so I'll I'll show plots with confidence intervals. Okay, so um we'll do this partly with just the uh iid. We'll do Clapper Pearson sort of standard interval analysis here. Uh so when you see a plot like this and I'm talking about a checkpoint versus another checkpoint, there'll be a mean and there'll be the 95% confidence intervals upper bound and lower bound. So if I want to say something like I made a I made a substantial difference checkpoint A is better than checkpoint B then a sufficient condition for that is that you know this checkpoint is safely above this this lower point. Sufficient but not necessary condition. Okay, but let me just I I said checkpoint for the first time there. That just it means, you know, one of the models, you know, one of the sets of parameters in my model training. I've said this I'm going to pull this checkpoint off. And I think it's really important to acknowledge that the statistics I have are about checkpoints, not about architectures. I'd love to say that, you know, flow policy is better than diffusion policy or vice versa, right? But that's actually a much harder statement to make and we often don't spend enough compute to make that. There are many sources of randomness in the pipeline. Of course, different objects, environments, initial conditions. Even the diffusion being a stochastic policy, right? It's going to do different rollouts if you because we want it to be uh we encourage ours our diffuser to be um deninoiser to be stoastic. But though the big one is that there's randomness in training, right? And uh you know, we're including this, but it's very expensive. At some point we're going to have to just bite the bullet and do that and capture all of our to make these statements. Uh but it's very expensive and it's going to require training a bazillion policies to do that. And the language you know the the computer vision community has done this. That's how you know if you think ResNet just popped out um you know you don't you're not acknowledging how many pol you know how many resets they train to make that happen. And the language community has done this and the robotics community is going to have to do this at some point but we haven't done it yet. Um, and then also as you'll see in some of the plots, there is a high variability across tasks. There's some tasks that things work very well, some tasks they don't work well. Sometimes I have a clear explanation for why, sometimes I don't. Right? So, we have to acknowledge that the choices I made uh about which tasks are affect my statistics. Okay? And that's not captured. Okay. How many rollouts do we need to start making statistical arguments? uh if I have n roll outs and I talk about an empirical mean of my berni process then the mean is always just this the success rate p okay but the variance goes like p * 1 - p / n right so if I make a plot that goes from success rate 0 to 50% it's symmetric on the other side I could have plotted uh the whole thing but it's symmetric since it's p * 1 - paid what is the width of that confidence interval. And unfortunately, if you only do 10 rollouts, then the width in the worst case is like 60ome percent. Right? So if you're trying to say that checkpoint A is better than checkpoint B and you've run 10 rollouts, it's like I don't I don't know how you fit that in 100 points, right? If you do 20 rollouts, you know, you get better, but and it's a trade-off. It's so expensive to do these, right? So we have decided for the single task results we'll show today we're we'll budget 50 rollouts to get ourselves down into this sort of smaller confidence interval. Okay. And in sim we can do many more. That's for a single task. Things get more powerful when you start doing multitask. So our real multitask and our sim multitask. That's when we can start talking about you know averaging over all of the tasks where we start getting our small confidence intervals and nice separation claims. Okay. I'm sorry to harp on it, but this is like it's a painful reality of doing these tests. Okay, so um is checkpoint A better than checkpoint B? So a sufficient condition is that they separate, but it's not a necessary condition. You can do better um with some adaptive testing. Uh you have to really watch out for these things. So um uh you know, if you know about P hacking, P hacking is a real danger in this uh in this world. It would be very tempting to like run some number of experiments, see if your confidence intervals separate. If they don't, run some more experiments. And you're not allowed to do that. That's P hacking, right? Um, so you have to come up with the policy, basically an adaptive testing framework that allows you to with 95% confidence, my policy A is better than policy B. The mean is is above, for instance. Um, and so the other way I'll show because the confidence intervals don't tell the whole story, the other way I'll show some of these results is as with these basically plotting the posterior of a Beijian analysis that shows the distribution and then you can start to see there's a whole distribution uh uh of possible outcomes here and sometimes those distributions even if they're tails overlap you can still with probability separate them. So this common letter display is one way to say that. So if they if it says AA for instance that means they're in the same equivalence class they were you couldn't separate them but if it says AB that means there was a with statistical confidence we can say those are there's a strict ordering in those performance okay now here's um maybe a philosophical point that I want to um make here is so we're in the low data regime uh we're doing 200 demonstrations per task Gemini robotics for instance does many many more we've ever done that? Um the tasks that we're showing are intentionally difficult. Like the the initial conditions are crazy in simulation. Sometimes we have cameras that are moving on every different roll out. Okay, we make these things intentionally hard. Uh why? If you're trying to understand the policy, the changes that you're making in the policies, well, let me say it the other way. If I was trying to impress you with my success rate, I would not do that, right? But if I really just sincerely want to learn about the policies, then the right thing to do is to stay away from those tales where you lose information and put yourself in the middle of the distribution and run lots of tests. That's the way that's the place where you learn the most. Okay? But it means I have to stand in front of you and show you some of my skills have 20% success rate and some of them have 80% success rate and I'm proud of that. Okay? But you just have to give up because I don't care how good your policy is. You can do adaptive stress testing. I mean we used in driving again we used to talk about the simulator should be like driving in downtown New Delhi all the time because that's how you learn you put the hardest possible conditions right okay and I think really maybe just if you this whole idea of trying to you know I I have to convince myself in order to convince you I think you really have to live that like there's a thousand small decisions that were made in the course of of creating the results I'm about to show you we oh we found a small thing that called bug or something we should have done better and every time you hit one of those always there's a strict decision it basically do we rerun the do we retrain the policy do we rerun the evals right so if it makes LBM stronger we don't do it just because we don't have time if it makes the single task stronger the single task is the opponent then you must run it right every advantage has to be given to the the weaker player okay so so we've made a thousand little small cuts that slowly cut away at the LBM performance just so I'm confident to say that what we what we've said here and there's just no way that the single task is going to be better you know to the best of our abilities with 95% confidence. Okay, so let's dig into some uh of the results here. First some highle observations, right? Um there are some architectural changes that just help like you know especially as you go to the multitask the bigger encoders uh just just work better. DIT scales better than UNET the relative action parameterization that um I think was first published in the UMI paper. It really does do better statistically. But a lot of the things that we thought were important, a lot of the architecture changes that we spent a lot of time talking about and were very excited about ended up actually disappearing in the noise. Um, we were pretty convinced for a little while and then you keep running the test and it's like, oh, that one's gone, you know. Um, okay. Yeah, we've often convinced ourselves something improved until all the tests came back. Um, and I have to say some of the changes we make and as we get better at these games, right, we're going to get to the point where we're going to try to say this architecture change or this data change made me 1% better. That is very hard claim to make today. If it's a 10-point change, we're good. If it's a onepoint change, we're going to have to change our, you know, we're going to have to get bigger data to do that. Um, multitask pre-training does safely cut through the noise, right? A lot of our things don't matter. Multitask stands out. It really does make a big difference. Okay? And actually, a lot of other engineering details do. Nobody really wants to write a paper about data normalization, but it's like 40 times more important than architecture or something like that, right? I mean, really, it was it was actually like 40 points better than the difference between DP and flow and flow policy for us. Okay, so um let's go through through some of the specific results. So just as a warm-up, you know, one of the things we wanted to make sure was correct was that our slightly scaled up multitask diffusion policy could learn all the tasks. Okay, so just the experiment here is we're going to take our pre-trained LBM and we're going to we have you know 200 demonstrations approximately, sometimes more, sometimes less on all the tasks, okay? And um and we'll use those same 200 to train a diffusion policy on the tasks. And um these are scene tasks. So you know it's attempting to call it zero shot, but it's it's in the pre-training mix. It's just not in there's no there's no post-training. We're just going to take our LBM pre-training and apply it immediately to the tasks that were in distribution but with new initial conditions. Okay. And I want to compare that versus I just took one at a time 200 demonstrations and trained a single diffusion policy. Okay. Okay. And here's the violin plots popping up. And I and I put in more detail than you could possibly parse just so you can see some of the complexity here. So for lots of skills, I would have been happy, by the way, if this just was flat. If everything was flat, I would say I my multitask policy learned as much as my single task policies. Okay. In fact, it's more nuanced. We see some policies the the pre-trained multitask is just better, which is kind of what we hoped that we'd see some transfer. Lots of times they're similar. Sometimes it's worse. Okay. Um again, like this is this is again like a um you know, scientific method thing, but I I could have just disappeared these columns. You know, those tasks just never never appeared and my stats would go woof, you know, and you'd be much more impressed, right? Don't do that. Don't do it. Okay. Instead, what you do is every time you see something like that, that's gold. You go into your logs and you figure out what the heck is happening there. Uh the first one that stood out to me is this one hanging out at the bottom. Like what is that? Put banana on saucer. How is that? Like we're we're cutting apples. We're like, why is put banana on saucer down at the bottom? And it turns out it's because of my uh this is because it's the multitask uh diffusion policy. So, we go into our leaderboard, we pull out our video, and it turns out it just put the it always puts the orange on the saucer. We must have too much orange in our pre-training mix. So, it just like refuses to do the banana, picks up the orange every time, but it always puts something on the saucer. Okay. So, that's just really because the language steering is not good. Okay. Um, getting into the hypothesis. So, by training on many tasks, we can add new tasks more quickly. So I really wanted a plot where you say the number of skills goes up and then how many skills would it take how many demonstrations would it take to get to 80%. That was that is the right concept I think but that would take so many experiments to make that plot. That was not the right axis. Um so instead we're going to show something a little different but but making the same point. So now we're going to fine-tune into new tasks. So we'll take our pre-trained model. I've got a new task my 101th task if you will. I'm going to train against, you know, 200 demonstrations on that versus pre-training and then fine-tuning with 200 just like we talked about at the very beginning. And these are safely out of the pre-training distribution. Now, how do you get so we have literally been doing almost anything you can think of doing with two arms bolted to a table for the last two years? Like people have to get creative to come up with the next thing that you know that you haven't already done with two hands bolted to a table. It's two arms bolted to a table. So the way we did it, we said let's do some crazy apple cing for instance was one of our sort of crazy long horizon clearly not in our pre-training mix. So we came up with like clearly novel tasks just to make sure that we were safely unseen and these are the kind of plots that we're able to make now. So the axis here is if I use all 200 demonstrations or if I use just 100 demonstrations 50% of the data or 15% of the data. Okay. And the single task policy, you know, benefits a lot. They both benefit, of course, from from having more data. But what's interesting and consistent, very consistent across these different runs, okay, is that the the LD the pre-training allows you to do much better with less demonstrations. Okay, so we do see this kind of and we see a glide path. Um so LBMs the pre-training really do improve rapidly with less demonstrations on tasks that are it's not really about generalization here. This is about dexterity right this is about you know did I sheath the knife properly? Did I you know like are my IGEN values better and are my recoveries more pronounced you know these are like fundamental dynamics and control things that are happening because of the pre-training okay we have a bunch of tasks in sim even that were that are were intentionally very difficult and we include those and all our performance drops into like our you know please forgive my 25% success rate but the trends continue even for our different battery of of of tasks. We've done less of these in real, although they're actually running right now, the partial, but all of our real all of the real results we've gotten so far. You don't expect the absolute uh to to match the remember the philosophy here is you have to cons compare relative the total performance is task dependent but we see very consistent in terms of the relative gains of the pre-training in real versus in sim and um just to show that you know there's lots and lots of details here there's some really hard tasks that get them really low but the averages do separate I showed Okay. Uh and and in real we see this the same thing. What's interesting is uh the bike rotor install which I'll show at the end is um is actually one of the hardest tasks. Uh we we rarely get all the way to the end. The success rate is low but I mean like pi for instance has been using task completion as the metric and I think that's great. I think that is strong and it shows a lot more signal in something like the bike rotor where you or the apple cing where you you go you know very successfully chop an apple and then you miss the last step of sheathing the knife or something like that. So I will talk at the end about some of the how that affects sadly it doesn't change the statistical strength significantly. That's a very subtle argument. um we're going to try to write it up carefully uh in the draft, but um uh you might think that the statistical weakness is just because we're doing binary success failure, but so far continuous metrics are, you know, they tend to have lower variance, but they're also have closer means, so it takes just as many samples to separate them. And it's a subtle argument. I I was convinced it was going to help, but actually statistically, it's still a weak position. Hypothesis 2 was about robustness. Okay. So, I'm going to do the same experiments I just showed from the seen and unseen tasks, but this time we're going to go, at least in simulation, we're going to go really crazy. We're going to have, you know, lights changing, background changing, cameras moving on every roll out. Okay. Um, in in the real, we'll do less of that, but but still try to put novel objects in and the like. Uh but but really like the uh I didn't actually realize until fair surprisingly recently that actually our distribution shift on was moving the cameras as much as it's moving. It's really hard. Okay, which is why we get low low scores um but very consistent uh improvement from from un from uh the single task versus the the LBM. And I'd say this is probably the strongest result that we can that we we've seen is that consistently in these hard tasks distribution shift hard scenarios we see that um you know basically that LBMS achieve the pre-training allows us to achieve comparable performance to single task with 15% of the data. That seems more invariant than some of the other numbers we've seen. For instance, when we put in uh very difficult tasks, we still see, you know, somehow a lot of the performance even at 15% of the data. Okay. Um there's lots and lots of details that I could um I can go into. We've done things like distribution shift in hardware, measured similar things, trying to vary from the more demonstrations to less demonstrations. The trends are all pretty consistent. This one I singled out because it's actually just useful as a roboticist. So, um, we used to worry before that it would be if we did our demonstrations on station one, then we would do our rollouts on station one, right? You wouldn't try to train, you wouldn't often try to train a diffusion policy on one station and then apply it on a different station. Partly because we don't work super hard to make our stations identical. We just mount cameras where we mount cameras. We try not to bump the cameras, but we don't try to make things uh perfectly identical. And what we see is that maybe not surprisingly is that the pre-training helps you now deploy on new stations. Right? So this one uh the pre- the the single task baseline had four different stations collecting data and so it had a little bump a little improvement in performance. The one where the initial data collect was done on only two stations. The pre-training made a really big change to how much robustness you have if you go to a novel station. So it's just nice to finally feel like you could deploy on a different system. Yeah. Really interesting analysis about robustness. It seems like this is primarily visual robustness, right? Um how do you think about physical robustness? I think I've seen some of those LVM being able to like miss the graph of banana. So retry and how do you think you can measure things like that? So what makes you say that this is that's a great question. I should repeat it. So is this more about visual robustness than physical robustness? What makes you think it's visual robustness? I think earlier you described that you were hearing the lighting. Ah, I thought moving the cameras as much as we were is crazy. I shouldn't have overemphasized that we're also doing crazy things to the physics parameters. Yeah, it's really meant to be both. For your simulation experiments, you vary physical parameters. Yes. Yes. Sorry. We're we're we're randomizing all the things. Okay. Shape, geometry. Well, not shape maybe because it's they're they're meshes, but um but bas basically every parameter that you could try to randomize, we try to we try to randomize. Mhm. So it's intentionally meant to be interrogating physical robustness in addition to visual. In fact, I think the visual robustness is, this is maybe my bias as a control guy, uh maybe less like I think the the pre-trained image encoders are very strong. Um so you kind of out of the box get pretty good visual robustness. Um but that's probably my bias. That's probably my bias. Yeah. Uh okay. The funny thing is that really like I said it's it's the little things like the normalization you know like so I could tell you what first step normalization is you do um we do these relative action trajectories now so everything is in relative coordinates but it's not relative it's not like delta coordinates because then you'd worry about sort of drift we do it from from time zero we're going to we're going to predict 16 seconds 1.6 6 seconds into the future, we write all of the tra all of the action commands for those 16 seconds relative to time. We call it time one uh because let's call it time zero. It's clear. Okay. Um right. So the statistics of what your 16th step looks like relative to your first step are different. And for a long time just because it was obvious first thing to do, we were using the same normalizer for all of for that entire trajectory. And it's like a obvious simple thing just to compute the statistics differently independently for all of those steps into the future and it made like a 20 point difference in some of our results right and you you have to clip it in the right way. Those things again it's not what people write papers about but they make a huge difference. A couple other takeaways since I know we have a lot of domain experts around. um always in our diffusion I mean I think diffusion more generally but certainly in diffusion policy we know that validation loss the we have call it val action MSSE doesn't correspond to that's kind of like openloop prediction and openloop prediction is not predictive of closed loop performance so you you really have to run a simulation to know which model is best okay and in particular we tricked ourselves we were thought okay well at least for just checking you know so So we train let's say 100 checkpoints 100k steps we'll look at 100 90 80 and pick the best one. It's not always 100 which is bad and broken but that's fine. So so we'll just put that on the shelf for a second. Um but so let's say the the 90 has the best val action MSE. Okay that's not enough. You got to do the rollouts even to just pick which which checkpoint. Uh the data quality still matters even when you have the pre-training. We see that making sure so the way we talk to um uh the way we sort of do our demonstrations is say please think a bit about how the robot might fail and give some examples of recoveries and if you don't do that it's worse. Okay. uh even I think maybe in the limit where the pre-training is strong enough it won't make a difference but right now it still matters and beware of looking just at indistribution performance particularly in sim when evaluating the pre-training so it wasn't until we really started doing the more aggressive testing that we felt like we were making better decisions uh based on our sim based testing and mapping to real okay Um, I do think success rate is is a limiting thing, right? And this is real. I don't I think I actually think this is great. I've heard this a few times and I I think I think it's true. I think you you know the the robot teachers will say it, right? You kind of like that that policy is just better. Like it's just I can guarantee it's better, you know. Um, so here's just a time lapse, right, of that apple cing. Uh it's you know it's so when you see the camera moving like this that is a you know to show you that it's there's no cuts right it's it never jumps. Okay. Um and it's going to fail a bunch and you'll see the guy you know like it misses the the knife at the end. Okay. But what you see is that it just the LB version the pre-trained version just looks better. it like um when it does fail it I mean apart from the knife missing which is kind of forgivable in my opinion but it it's it never does something that looks like it always looks like manipulation right it's just always trying it's always doing something and there's something qualitatively good about what's happening and so if you dig in we can see this statistically in our in our rubrics it pauses less they complete tasks at a higher completion rate um we can make our our plots at all the failur uh and and see that it you know it gets farther into the task more often. These are the sort of seni plots where you you can talk about all the different ways that it fails and if you get all the way to the end and you're not red then you've succeeded and the single task version you know you just see lots more early exits right where it failed for some early part of the task and the the LBM fine-tuned these again are like the really hard long horizon tasks. So um yeah and so many many different ways to sort of see that you get you get a lot farther into the task uh and you do qualitatively better with the softer metrics uh with the LBMS. And so um you know Mark Rabber used to always have a slide at the end saying called it the unvarnished truth right so I thought I'd put the unvarnished truth in. So we actually just used our rubrics to look back through all of our videos and said what is the worst like the worst one we've ever seen and the single task poly so this is not cherry picked you know but it's but it's maybe a little unfortunate for diffusion policy it happened to drop and the bike rotor kind of went out of the out of the scene the very worst thing we saw in the bike rotor um for for the LBM fine-tuned uh you let's just watch it's It's almost endearing, I think. Um, so it failed. Okay. But it tried again. Okay. And this thing just keeps failing and keeps trying and trying and trying until it gets its arm stuck. Basically, it's like scene camera is wedged in the bike, you know, and then at some point someone said, "Okay, that's enough." You know, take it out of its misery. and and uh and that seems to be the story is it just keeps you know like a little bump there a little engine that could you know and in a in a way that um I think this is the story of pre-training is is somehow uh we get these things that just have more I think they have more it's very hard to quantify I wish I had a really rigorous way to quantify that they had that they were sort of a had more recovery be behaviors available um we're working on those ideas too. Okay, so um maybe this is one way to say it. So um this is actually something we've been saying for a while that GPT doesn't always get the right answer, right? But even the early versions of GPT, they always generated beautiful pros, right? And I think roughly what we're seeing is that, you know, LBMs don't always accomplish the task, but it always looks like manipulation. It really does. If you just put a So, we put random This was actually before it had ever seen the towel. These are like this was a novel objects thing. We just put random things in front of the the robot and we told it nothing, right? Or you can say like do something useful or or like um you know something that's an outer distribution. Do something useful is actually in Droid, but there's a um there you could give it like arbitrary tasks and it just starts playing like there's no other way to describe it. It just starts doing it. Never sits still. You can walk up and you could put a banana in front of it. It'll go Whoa. And grab the banana, you know, and then and it'll like do a handover with the banana, put it over there for no reason. It's not listening to you. It's just playing. And and that is, I think, the story is that we used to see, you know, it would do the right task or it would kind of get stuck and pause or maybe would go a little unstable off to the side. And now it's it doesn't always do the right task, but it always almost always does manipulation. And I would say maybe even philosophically that this is what we want to see from pre-training right so if you believe in RL post training I think people do then really the role for pre-training is to try to get 5 to 10% success rate on any possible task like that's what we want because if you can I mean the deepseek story was it solves the pre-trained model I mean the the base model uh was was solving competition math at 5% Okay. And once that happens, you just roll it out a few times and and increase the good ones and RL can do wonders, right? And so I think that we're going to see the same thing of course in manipulation probably in simulation first, but but this is coming for sure. So uh the first principle is you must not fool yourself. I really tried to give you a deep dive into multitask pre-training and pre present evidence supporting that we're going to put on archive very soon here. Simple thing was that LBMs can learn all the task. We haven't crippled our model in any silly way. um that learning skill they enable learning skills more quickly that they improve robustness and data normalization matters like of course but uh it still stands out to me as like I cannot believe that was the biggest change we ever made um I think as a field we need to acknowledge that we're in a very weak statistical position it's very hard to do rigorous statistics in at this scale right and so I We need to think about very clever ways to address that. Uh there's a lot more to do. And so I I'll look ahead just a little bit here at the end. I know that's a conclusion, but we do have a collaboration with Boston Dynamics now. It's pretty awesome. Those guys are are amazing. Their robot is amazing. Their controller is amazing. So the real results are coming soon. But I just want to give you a little sneak peek uh with what you can do with amazing whole body telly up. Right. So this is an early video of Electric Atlas. You can imagine just but it's a nice one to sort of show what you could do. Um but imagine if you've got a decades old uh MPC controller. I mean sorry something that's been matured over decades. It's new controller but it's something that's been matured for a long time. Um you know you can do really dynamic tasks. Uh this guy can jump. He can do all these things. I like this video because for some reason Kevin decided to do a pistol squat. Um, if you don't know what a pistol squat, you're about to see. Uh, and I don't know if Kevin also fell down, but the robot that was the NPC controller catching the robot, right? So, imagine having this blanket of protection around your f your Hey, that's Lucas. Yeah. So, yeah, just jumping around. Okay. So, and it's useful, too. you can kind of get into places you wouldn't have gotten into otherwise if you can really use your whole body. A lot of the humanoid imitation learning demonstrations right now are standing in front of a table and doing stuff. It's a little bit more than um tabletop manipulation. But uh so coming soon uh I'll take questions and maybe while I do I will uh just show you this bike rotor which is a my one of my favorite tasks here in the background. [Applause] Yes, thank you for the talk. Can you expand more about the notion of same task about unseen objects or I think still or combination? You're right. So in simulation it's very clean. We just we make a handful of scenarios a handful of tasks in each scenario. They're all intentionally different and we just subset them. Some of we're going to, you know, it's kind of our validation set scene, you know, train validation. Okay, it's very clean and we can we can look at at the description files and know that they're different. The reason it's more subtle and real is because we've been collecting all kinds of arbitrary data for a couple years now. And so I just we were more worried about saying like, okay, have we ever put a banana on a saucer before? I don't know. It could be h it could be hidden in the data. So in order to make something that was condition convincingly out of the scene the pre-training mix we did these we call them extravaganza you know p like the like this bike rotor thing. Yes. Um thank you for the great talk. Uh I was wondering um whether you have any tips or like uh finding learnings that you can like communicate to the field about how to create really good sim simulators that we can use to evaluate. and have 10 high quality ones. If everyone like contributes one, I feel like we could go really far away like simulation large scale simulation evaluation of many different tasks. Awesome. I'll repeat just for the the video or whatever, but um okay, so what can we do about uh or what can I say about making simulations? So, first of all, Chiron's been on me to release our our simulation based eval, and we have a beta version that's ready. So, I' I've been listening uh and Jeremy has and others have been working very hard. So, we're going to try to release ours. Um it has it has the one we'll release is probably our first four scenarios because we've hammered on on the most. It's easy to add more, but again, that's the place where we've really actually carefully curated the art and and whatever. Um I can say a lot about the simulation. I mean the uh so I mean maybe the highest level is that uh that uh Isaac and Mujoko are super fast. They're way better than Drake from for large scale parallel testing. Uh Drake has always been tried to be closer to real. So put more fidelity in the in the contact parameters. Um it's a little harder to use. Uh but uh uh but for you know now this is just you can just use it in the cloud. You just give me a checkpoint, it'll run. you don't have to understand Drake diagrams. Uh so I don't know how much that matters. I think uh you can domain randomize yourself out in RL training over sim fidelity issues. Eval is a little bit different. So I have the most experience with Drake. We've seen good correspondence, not perfect correspondence. Sometimes we think for instance that the simulator seems to line lean on the scene the wrist cameras a little bit more than the scene cameras compared to real. why we're trying to dig into, you know, there's there's little things that that are anomalies, but it's a pretty good indicator and we're going to keep making it better and better. Uh, so so I think if we look ahead, so LLM evals, people just collect evals, right? And and we should have that same thing here. So you should have hopefully you'll have LBM eval as one of your um choices and you'll run it every time you put a checkpoint on and you'll have some others and we'll see which ones correspond to the best, you know, correspondence with real. And I would love for you to add 10 more scenarios or any you know to to scale and my question is about the generalization over the skills and generalization over the distributions like how you quantify the importance of generalization data with different variables and different kind of tables and different set of conditions and how important or how how it is for the generalization for the new task. Very good. So, so what about generalization? So, um I mean there's great work I I think I don't know here but you know people have made the axes of generalization that you want to measure. Uh we haven't cut it up like that um and done specific tests. What we try to do is make a single outofd distribution generalization test which changes all these things and that we want to probe I think we're probably probing all the same axes that that other people are um sometimes harder like larger distributions but we have not um made separate plots for each axis we could it's an interesting question yeah what do you see as the role for more like model based techniques going forward I always get this question. So this what is the role for modelbased techniques? So um so there are plenty of places where modelbased techniques could snap into the pipeline for data generation for uh analysis things like this but I want to give you a higher level answer. Okay. So um I'm still working on model based things too. um even if they don't aren't going to be the things that run on the robot. So the problem with the work I feel that that's happening here in my for me personally just as just me Russ as a person um we're very early stages. It's very messy. It's very hard to think deeply and ask narrow questions that you get concise answers. the modelbased is so much more mature and I think people should experience the maturity of of thought that can go with some of those methods and I worry that that not everybody will so um so I think that's a really important I think we should expect these worlds to come together in terms not not in terms of like um you know we're going to use sums of squares or whatever that might be different but leaponov functions are leapon functions stability is stability robustness is robustness those things are coming together. They're different paths up the same mountain or something. I don't know. Um but I really the risk here is that uh it I mean it's the opportunity is that we've opened up new vistas, whole new questions, lots of incremental things that we have to do to kind of make progress on that. But that is a very different experience, intellectual experience than really deeply understanding a mountain of of knowledge and and trying to find a way to advance. I think both are super valuable. Yeah. Yes. So after all the sim and real eval that you've done, how much do you trust simulation as an evaluation tool? Like if you had to choose between model A and model B, how are you being like you only need to test this? Great. Yeah. So how much can we actually just test simulation? Yeah. So um I the first answer is I think we must test trust simulation because oh my god re real world eval is so painful it is so expensive it's so stressful is like oh my god it can't be the answer you know maybe there there's I've heard clever ideas about crowdsourcing this and whatever I think there could be ways that you bust that open but it is painful it is painful so we must trust simulation I would rather make a mistake because my simulator was a little different then put everybody through what has been through in the last few months. Okay. Um so that's one answer. Second answer is that I I do think um there are many things where it does transfer like a lot of our stats the ones we showed you know that they really do you see a performance jump in simulation we we expect and see the same performance jump there are still cases where it's like why you know why did it do that and it's just a constant I mean the same thing happened when we were working on RL for instance um you have a physics engine it's pretty hardened uh but that RL will like find a way to beat your sim like to to put your simulator in a I remember we were doing you know putting plates in the dish rack and there was some like instability in the simulator. You could basically play tiddly winks with the plate and it would go into this, you know, and RL found that and that was annoying and then you make your simulator better, you know, and then you do that enough times and you whack down all the all the things that pop up uh and you get there. So we're we're pretty far on that journey but not all the way. So I I think um already most of our work most of our decisions are made in simulation and we just post hawk check and I think we'll get to the point very soon where we just you know we really really trust it. Yeah, I think it was really interesting discussion about robustness and recovery behaviors. In my opinion, I think the recovery behavior is most interesting because it's actually like say visual robustness, you're still doing the same actions and hopefully it should still work or if it's like different friction, you still mostly do the same thing, hopefully it still works. But recovery behavior sounds like an entirely different kind of beast. How do you measure if your system does recovery behaviors and how do you make sure you listed it the right amount without like intentionally collecting bad demos? Oh, we might do better intentionally like more than a shift. Okay, that's that was that last part was a was a great but maybe slightly separate question. So the first the first thing about so the question is about recovery behaviors and how do we um study them? I mean first of all you can we don't actually ask our teachers to label them as it happens. Maybe we should, but right now we're doing that as post hawk analysis and we're trying to find clever ways and ideas are welcome to to to sort of compare to say was that recovery something that came from this skill or did that come from another skill or you know trying to find what's the most similar in the right feature space you know and and thinking about things like that okay um the recovery happen even in the single skill they happen more often in the in the um LDM pre-trained we think but have not yet been able to prove just because there's these questions is whether they're coming from more diverse sources when they recover. Um okay and then maybe the second question was how do you teach recovery behaviors? So um we have traditionally done what we call batch dagger which is um so so we'll collect the first 200 demonstrations or whatever and then if we see it recover so and we try to say think about ways it might have failed and so like you know maybe you dropped the um the wrench so put it in a place where you might not have set it initially but it might have fallen pick it up from there and give me a demonstration that includes that. Okay, but in but in particular after it's after we've trained the first first policy we would say okay we've seen it fail in a handful of ways give me new demonstrations recovering from that failure the more elegant version of that which is online but not in this data set is to sort of do constant recovery during evaluation um right HG dagger for instance right uh so that's all yeah I think that that's all fairly good uh we aren't as careful as we could be about cutting out failures from our data. They are just sitting there in the pre-training mix. We hope they are drowned out by being rare, you know, like you fail in different ways. But we could try to when we have in the past tried to go in and look out and like, you know, sort of dissect out the failures. Uh it's a slippery slope, you know, because what what what is actually a failure? What do we cut? You know, so so we don't do that right now. Yeah. for us. I have two questions. Okay. The first one is you have more than 200 apples. Who ate them? Uh, who ate the apples? I ate a few for sure. Uh, yeah, you can only eat so many apples, right? Yeah, there was some We tried not to make food waste. There was some food waste that day. Yeah. Uh my second question which is related is um do you see a world where simal can be applied to all kind of scenarios like formable objects within or with apples? Yes. Yes. So um do we see a world where where simulations there's no question I think we we already know how to simulate all those things. We don't know how to simulate them with high fidelity at real-time rates, but that's coming. Um, yes. So, it's coming to Drake's very specifically. Maybe not the Apple's not today, but the all all the the liquids and deformables are in the pipeline and and coming out very soon, and we're being adding they're adding them now to our eval pipeline. So, so absolutely that's coming. Uh, the diversity is the real question. I think for fidelity is not the the thing that I worry about in SIM. I think we know how to simulate. I mean I Zuchen who's one of on our team his thesis was like simulating bread you know warming up in the oven and splitting and it was a multiarticle method I mean you could simulate anything okay but getting the diversity of the simulation is the real is the real question and that's where I think all the great advances in real to sim um are going to really really help so uh yeah I think that it has to be the it has to be uh we have to make that work Yes. Are there changes in robot design that you'd like to see? Oh jeez. So, I'll tell you what. Um, the question is about hardware design. So, one of the things that Aloha definitely got right that these arms just and I know you know this. You've already done the work on this, but I I just hate that we don't have a wrist here. It it is like these top down manipulators, hands down, they just get very limited because the last link is so darn long. Now, you've you've already had solutions to this and we should be using them. the um you know what else? I mean of course the dextrous hands are coming. Um I think having more clever uh endectors that allow in hand reorientation without fingers that's I think that's awesome. The question of course there is the total generality of it is you know did you make some tasks easier other hands other tasks harder. The um you know the only real argument in favor of fully dextrous hands is just that we've seen humans do all the tasks we care about roughly. Uh so uh the but the rate of progress in lowcost Dexter's hands is phenomenal. So that's you know you won't see only these on our robots that for very much longer. But what about force control and compliance? Okay. So uh force control and compliance. So we have uh we're in a joint impedance control and then we have a diff IK on top of that. We were talking earlier that we could have done that with operation space all the way. Sorry. Um so but just maybe historically because they were we were using the Franco stack uh impedance controller first. The joint impedance controller is essential to making these things work. We have seen tasks like the one I I think the most about is the one of the early egg beater tasks. So we holding one hand and then we have this kinematic closed loop but with a weird kinematic constraint. First of all the human needed to get haptic feedback in order to feel that and operate it and the joint impedance controller was absolutely saving us and allowing that to to execute. We do collision avoidance, so we don't, you know, if you, if you notice, there's no scuff marks on the arms. We're pretty good. Even our policy has is is being protected with collision avoidance low-level stuff. Uh, and the table also in the known, but once we go mobile, that stuff gets harder. Um, so I so we're benefiting a lot from that. Do we need it in the fingers? I don't know. I don't know. Of course, it's it's really a bandwidth. I mean, you you know this. So, so it's really a bandwidth thing. You can you can act like a force source with position but um the question is really I guess what's the right space to do your high frequency control. These policies are low bandwidth. So um you know the diffusion policy is thinking about uh 10 hertz but it's only being doing inference at about one hertz less a little more than one hertz. So we're not in a force control regime with the big policies. We're relying on our low-level controller to do something like that. Thank you. Yeah. Uh so with all the impressive result you showed today, how far do you think we are from seeing robots actually be helpful at people's homes? Uh and what are the major bottleneck or obstacles that we're going to resolve? Good question. So how far are we from robots getting into homes? Okay, so the thing that's just amazing and has just has really I mean I I couldn't have seen it coming as fast as it's coming is that we're we're all going to have these foundation models for manipulation, right? and we're all gonna have humanoids or whatever the better design is, right? Um, so but it's still a really really long path to having a product in the home. I think maybe people don't remember how hard it is to actually like let's be more specific. Okay. Ignore hardware capability. If we just focus on main production capability, do you think we have the recipe? Yeah, I see. So you're factoring out cost and uh and and hardware and stuff like this in terms of manipul I think I think we're on the path. I think the PI 0.5 looked awesome. I think um I don't see a ceiling for what we can do with the large scale pre-training but I think it is probably there and we'll have to have I mean I think I believe we're going to have with the current recipes small changes on the current recipes robots that are actually useful like I think we have enough to kind of get to the next round and then we'll have more robots out there but I don't think we'll we've solved it completely or maybe just for intellectual for the journey I hope there's another ceiling that we have to like think our way around. Um, but I do feel like the stuff I've seen, you know, the stuff that we read and see on YouTube and Twitter, um, is going to go a long way. And I think we we'll have to be inventive about finding applications where you don't have to succeed at 100% success rate. Like we're not going to start probably with like open heart surgery, you know, u, but maybe putting away the towels is okay or like I don't know, picking fruit or something like that. those seem safe um right that if we are inventive as a community about finding those applications I do think we have stuff that's going to get us there but the supply chain matters and the cost matters and those are those are doozies. Yeah. Yeah. Sure. Thank you for the wonderful talk. Uh from the videos you've shown today it's it seems like visual motor policy can push robots really far. I'm curious if you think with scale that's pretty much going to solve manipulation or could there be other sensing modalities that can be critical? Good question. Okay. So, um I almost never use the word solve. I think there will always be a next round. Um if if only our understanding I think our understanding here is so young, it's so naive. Um at some point we'll have theorems that explain this stuff. We don't have those yet, right? I think well, so first we have this sort of like very um well, you can look at the history of science. Okay, so maybe electricity and magnetism. So we're like Volulta was sticking electrodes in frogs, right? I think we're in the electrodes and frogs stage. It's pretty good. And then and then like Faraday came along and and did really the right empirical science and kind of clarified the situation dove in found exactly the right quantities and then it took a number of years later that we had Maxwell's equations that sort of cleaned it all up. I I think the fact that this people I mean people often say this is just so much more complicated. It's all about the data. It's so big. I think that's just because we don't understand it yet. I think we'll they're going to look different but we're going to have the laws. We're going to have the principles. Maybe it's about the the the way you do a curriculum or whatever. We're going to have the core ideas and we don't have that yet. It's going to take a long time to get there. We'll also have tactile sensors if that's what you're asking. Okay. Yes. Yes. Yes. Sorry to kind of go philosophical on you there. Yes. What might be the next ceiling in your the next ceiling? Yeah. Um Yeah. Yeah, I mean I think I think we have a pretty clear glide path right now to just keep making these things better, but um I I don't know what the next honestly. I think hardware is limiting at some point. Our current hardware is limiting, but I wish I had a more crisp answer to request. I I I do think I feel limited often by the uh by the hardware. Yeah, sure. I have a question about human computer um traction. So like how safe are the robots currently and what best make them more safer to allow them to enter the home and other Awesome. So how safe are the robots? How how do we get into the home? The robot the safety case on this robot is uh based on the low-level controller. It's basically this joint impedance controller. It has a a safety like it won't hit you too hard. It could pick up a knife very slowly and you know and do so but but there's a safety case around the physical contact force, right? That is a poor, you know, that's not all everything you need, but it's a start. Uh, that gets even harder when you're a humanoid robot or or even a wheeled robot moving around the home. You proving that I'll never tip over is a pretty tough thing, right? Um, I mean, I think 1X is awesome. They're they're building very lightweight robots. Thinking safety from the from the hardware principles from the very beginning. I think we'll see more things like that. There's a hardware way to sort of work your way out of that. Having more data with people around is uh is definitely on the road map. I think that's going to help just being more uh intuitive of you know meeting people's expectations about how to perform when those things those situations happen. But my prediction would be that the first robots in the home have a safety case that's something like if a person comes within 10 feet of me I'm going to turn off and then we'll go we'll get better and better you know and but that could still be useful. That could still get robots out there being useful. Maybe it only goes in your apartment when you're out, but it's clean when you come back. I don't know. There's something like that. Yeah, this might be a question is so sequential and these models grow larger tasks. Um, do you see there being a way to speed up real time like besides just throwing more at it? Okay. So what is so how do we do more dynamic tasks for instance like and have the robots moving faster? Is that kind of the question? I think um I think the we've seen some examples of people running policies that are architecturally similar to this at closer to like walking balancing rates. Um I think to do it with a full visual motor policy requires some engineering. um you know warm starting keeping your context alive uh you know quantization all but we kind of know how to do all that stuff uh it's just a matter of prioritizing it the maybe surprising thing not surprising but the the the task we found a lot of tasks like we don't often feel limited by our control bandwidth like most of the tasks we're doing around I mean if we wanted to catch a ro a ball being thrown at it that would be tough throwing a ball is okay catching a balls is tougher right? Um with at this control rate. Um or balancing a humanoid at this control rate would be probably a bad idea. Uh but I I think maybe there's fundamentally fundament I hope there's fundamentally new architectures that will make this even better. But I don't think I think if we didn't have them, we could just do better engineering on the ones we have and get pretty close. Yeah. What are some lessons or to some of the missions you have about quantifying recovery behavior or recovery behavior? So yes, how do we quantify recovery behavior? So I think right now the objectives are these slightly surrogate objectives about how far does it get into the task, how many times does it fail? Um we're working on like post hawk. We've done small amounts like small chunks of our data humans have gone through and done extra annotations and the like. We're trying to automate that pipeline so we just always have that. Um I think that is going to open up the ability to sort of really measure these things for the whole data set but we're still limited on our sort of counting and probing of those recovery behaviors. I'm actually very excited about this and the thing I said before about values and and and recovery like this task for example is measurably better. Um let me just you know play it's measurably better but actually not because of recovery maneuvers so much. it it somehow just makes it through the task more often without making a mistake. So there's something about the rollouts that are kind of more stable in my mind. That's kind of a continuous stability, a local stability. The apple cing it has for instance more recovery that's more of a discrete stability, a second layer of stability. I think both are happening but really I think we need to keep thinking about how to probe that and understand that and connect it to Leoponov and other things right I I think there's there's a story there yes so the low-level controller basically protects us from singularities um we have a diffic layer that basically um does the even if you do drive it into I mean we do drive it into singularity sometimes but it tries We're getting better and better. Sometimes we have to improve it, but it does at least natural things when it gets to joint limits or singularities. Singularity or does it uh I would like to say it never enters a singularity, but that's not true. And uh and sometimes it recovers and sometimes it gets restarted, but u but it was pretty good. I mean the the state it's certainly not the policy that's keeping it out of singularity, it's the it's the interface. Okay. Yes. Um you mentioned early on in your talk that you were interested in video generation models. Yeah. Um I'm curious how you think the improvement is in video generation models as long as you're thinking of what data we should be collecting. Oh okay. Yeah. So how do video generative models sort of change everything? Um okay there's a question of like how do how would you use if you had a world model let's say an action condition video model. Um could you use that for eval? Could you use that for data gen? Could you use that as on time, you know, test time inference or or, you know, execution time inference? Um, probably all of those things. The first thing though for me is just having the uh having a different intermediate representation that was trained on these objectives to sort of predict future robot videos and asking is that a fundamentally different, you know, maybe more 3D aware, maybe, you know, somehow a different representation to start of from as a base uh a base layer. So, I think that's coming. Uh, I don't know how it's going to compare with the VA yet. We're going to find out soon. Um, uh, it's a it's definitely a different angle. In the long run, I think internet data will win. I think probably we need a base of of robot data so that you can um know what force feels like and and sort of know the details of your robot. Uh but the common sense about the world once we can build a bridge from our robot data into like watching human videos that's like such a such a huge surplus of data that probably that's going to be you know maybe we'll need 1% robot data and 99% everything else in the long run. But I do think we have to build that first foundation. I don't I think if I was watching a h you do it video on YouTube um and I had never touched the world I think I would probably not get as much out of it as if I'm you know had a lifetime of touching the world and now I watch the YouTube video. It seems like you're being very careful about understanding what the model needs to perform well and how are you going to go about understanding what is like data to collect when you have all of this video data available. Okay. So how do we I think how do we understand the right value of data I mean I think already even in the robot data I mean the language world just to take a story one of the stories that I'm familiar with um Ludvik is he he led a project called data comp lm for instance right so you take you you fix the architecture you fix the training recipe you fix everything and then you just start subsetting the data and the game is what what filters can you put on the data to get the strongest performance out for your language um tasks. Okay. Uh, and you, it turns out, I think, you know, if you're similar to in Wikipedia or like Reddit, are you smarter than a sixth grader? I think, you know, that's a good classifier to like find. So, there's something there's a cool story like that. We're going to have the same thing uh for for manipulation. I think we'll we'll first collect a bunch of data. It'll be expensive. Uh, and then we'll start doing the ablations and it's going to cost a fortune, but we'll start learning the lessons of what what data is valuable empirically. I don't know a first principles way to do it. Faraday will come along soon and Maxwell after that, but first it's going to just be like a lot of GPU. I think that's I maybe you have a better way, but